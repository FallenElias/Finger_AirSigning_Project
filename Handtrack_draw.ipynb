{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run on airwriting venv py 3.9.16 prooved\n",
    "# All imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import DPTForDepthEstimation, DPTFeatureExtractor\n",
    "import mediapipe as mp\n",
    "from torchvision.transforms.functional import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordVideo(output) :\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Define the codec and create a VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use appropriate codec for your system\n",
    "    out = cv2.VideoWriter(output, fourcc, 30.0, (640, 480))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            out.write(frame)\n",
    "            \n",
    "            # Write the frame to the output file\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Display the resulting frame\n",
    "            cv2.imshow('frame', frame)\n",
    "\n",
    "            # Exit recording if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_depthVideo(video_path, model, feature_extractor, batch_size=4):\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    video_path = video_path.split('\\\\')[0] + \"\\depth_video.mp4\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(video_path, fourcc, 30, (640, 480))\n",
    "    \n",
    "    try:\n",
    "        batch = []\n",
    "        for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "            is_read, frame = cap.read()\n",
    "            if not is_read:\n",
    "                break\n",
    "            image = Image.fromarray(frame)\n",
    "            pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
    "            batch.append(pixel_values)\n",
    "\n",
    "            if len(batch) == batch_size:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(torch.cat(batch, dim=0))\n",
    "                    #outputs = model(batch[0])\n",
    "                predicted_depths = outputs.predicted_depth\n",
    "\n",
    "                predictions = torch.nn.functional.interpolate(\n",
    "                    predicted_depths.unsqueeze(1),\n",
    "                    size=image.size[::-1],\n",
    "                    mode=\"bicubic\",\n",
    "                    align_corners=False,\n",
    "                ) \n",
    "                               \n",
    "                predictions = torch.split(predictions, split_size_or_sections=1, dim=0)\n",
    "\n",
    "                for pred in predictions:\n",
    "                    output = pred.squeeze().cpu().numpy()\n",
    "                    formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "                    img = Image.fromarray(formatted)\n",
    "                    img.save('img.jpg')\n",
    "                    image = cv2.imread('img.jpg')\n",
    "                    video.write(image)                \n",
    "                batch = []\n",
    "\n",
    "        # Traiter le dernier batch\n",
    "        if len(batch) > 0:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(torch.cat(batch, dim=0))\n",
    "                predicted_depths = outputs.predicted_depth\n",
    "\n",
    "                predictions = torch.nn.functional.interpolate(\n",
    "                        predicted_depths.unsqueeze(1),\n",
    "                        size=image.size[::-1],\n",
    "                        mode=\"bicubic\",\n",
    "                        align_corners=False,\n",
    "                    ) \n",
    "                                \n",
    "                predictions = torch.split(predictions, split_size_or_sections=1, dim=0)\n",
    "\n",
    "                for pred in predictions:\n",
    "                    output = pred.squeeze().cpu().numpy()\n",
    "                    formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "                    img = Image.fromarray(formatted)\n",
    "                    img.save('img.jpg')\n",
    "                    image = cv2.imread('img.jpg')\n",
    "                    video.write(image)    \n",
    "    finally:\n",
    "        cap.release()\n",
    "        video.release()\n",
    "        print(\"La vidéo a été créée avec succès !\")\n",
    "    \n",
    "    return video_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_tracking(original_video_path, depth_video_path):\n",
    "    # sourcery skip: low-code-quality\n",
    "        #finger sur une vidéo\n",
    "\n",
    "    # Set up video capture from default camera\n",
    "    cap = cv2.VideoCapture(original_video_path)\n",
    "    depth = cv2.VideoCapture(depth_video_path)\n",
    "\n",
    "    # Set up MediaPipe hand detection\n",
    "    mpHands = mp.solutions.hands\n",
    "    hands = mpHands.Hands()\n",
    "    mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "    # Initialize list of finger positions\n",
    "    finger_positions = []\n",
    "    images = []\n",
    "    # Main loop for video capture and hand detection\n",
    "    imageCompteur = 0\n",
    "    while cap.isOpened():\n",
    "        # Capture a frame from the camera\n",
    "        success, image = cap.read()\n",
    "\n",
    "        # Check if the frame was successfully read\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        if depth.isOpened():\n",
    "            # Capture a frame from the camera\n",
    "            success_depth, image_depth = depth.read()\n",
    "\n",
    "            # Check if the frame was successfully read\n",
    "            if not success_depth:\n",
    "                break\n",
    "\n",
    "            image = cv2.flip(image, 1)\n",
    "            image_depth = cv2.flip(image_depth, 1)\n",
    "\n",
    "            mask = image_depth > 220\n",
    "            sumMask = sum(sum(sum(mask))) \n",
    "\n",
    "            # Convert the color space of the image from BGR to RGB\n",
    "            imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Use MediaPipe to detect hand landmarks in the image\n",
    "            results = hands.process(imageRGB)\n",
    "\n",
    "            # Check if any hands were detected in the image\n",
    "            if results.multi_hand_landmarks:\n",
    "                # Loop through all detected hands\n",
    "                for handLms in results.multi_hand_landmarks:\n",
    "                    # Loop through all the landmarks of the current hand\n",
    "                    hasHeight = False\n",
    "                    for id, lm in enumerate(handLms.landmark):\n",
    "                        # Get the pixel coordinates of the landmark\n",
    "                        h, w, c = image.shape\n",
    "                        cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "\n",
    "                        # If the current landmark is the tip of the index finger, add its position to the list\n",
    "                        if id == 8 :\n",
    "                            hasHeight = True\n",
    "                            if sumMask> 115000 : #image_depth[cy][cx][0] > 220 :\n",
    "                                finger_positions.append((cx, cy))\n",
    "                                cv2.circle(image, (cx, cy), 10, (255, 0, 255), cv2.FILLED)\n",
    "                                imageCompteur = 0 \n",
    "                            else : \n",
    "                                imageCompteur +=1\n",
    "\n",
    "                    if not hasHeight :\n",
    "                        imageCompteur +=1\n",
    "                    if imageCompteur >= 5 :\n",
    "                        finger_positions.append(\"stop\")\n",
    "                        imageCompteur = 0\n",
    "\n",
    "                    # Draw the landmarks and connections on the image using MediaPipe\n",
    "                    mpDraw.draw_landmarks(image, handLms, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "            # If there are any finger positions in the list, draw a curve passing through all of them\n",
    "            finger_positions.append(\"stop\")\n",
    "            if finger_positions:\n",
    "                #split array\n",
    "                curve = []\n",
    "                for i in finger_positions :\n",
    "                    if i != \"stop\" :\n",
    "                        curve.append(i)\n",
    "                    elif len(curve) > 0:\n",
    "                        curve = np.array(curve)\n",
    "                        cv2.polylines(image, [curve], False, (255, 0, 0), 3)\n",
    "                        curve = []\n",
    "            finger_positions.pop()\n",
    "\n",
    "            # Display the image on the screen\n",
    "            cv2.imshow(\"Output\", image)\n",
    "            images.append(image)\n",
    "\n",
    "            # Check for the Esc key to stop the program\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    savePath = depth_video_path.split('\\\\')[0] + \"\\\\tracking.mp4\"\n",
    "\n",
    "    out = cv2.VideoWriter(savePath, fourcc, 30, (640, 480))\n",
    "    print(f\"video save at: {savePath}\")\n",
    "\n",
    "    # Main loop for writing video\n",
    "    for img in images:\n",
    "        out.write(img)\n",
    "\n",
    "    out.release()\n",
    "    # Release the video capture object and close all windows\n",
    "    cap.release()\n",
    "    depth.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\", cache_dir=\"models/\")\n",
    "feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-hybrid-midas\", cache_dir=\"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La vidéo a été créée avec succès !\n",
      "video save at: video\\tracking.mp4\n"
     ]
    }
   ],
   "source": [
    "#main \n",
    "\n",
    "output = 'video\\init.mp4'   \n",
    "recordVideo(output)\n",
    "video_path = video_to_depthVideo(output, model, feature_extractor, 32)\n",
    "video_tracking(output, video_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
