{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run on airwriting venv py 3.9.16 prooved\n",
    "# All imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import DPTForDepthEstimation, DPTFeatureExtractor\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordVideo(output) :\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Define the codec and create a VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use appropriate codec for your system\n",
    "    out = cv2.VideoWriter(output, fourcc, 30.0, (640, 480))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            out.write(frame)\n",
    "            \n",
    "            # Write the frame to the output file\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Display the resulting frame\n",
    "            cv2.imshow('frame', frame)\n",
    "\n",
    "            # Exit recording if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_frames(video_path):\n",
    "    #video_path = 0 pour le flux caméra en direct\n",
    "    if type(video_path)== str:\n",
    "        dirname, _ = os.path.splitext(video_path)\n",
    "        dirname += \"-opencv\"\n",
    "        \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)\n",
    "\n",
    "    count = 0        \n",
    "    while True :\n",
    "        is_read, frame = cap.read()\n",
    "\n",
    "        if not is_read:\n",
    "            break\n",
    "        im_name = dirname+'/frame_'+str(count)+'.jpg'\n",
    "        cv2.imwrite(im_name, frame)\n",
    "        count+=1\n",
    "\n",
    "    return dirname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_to_depth_frame (path):\n",
    "    \n",
    "    model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\", cache_dir = \"models/\")\n",
    "    feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-hybrid-midas\", cache_dir = \"models/\")\n",
    "\n",
    "    dir_list = os.listdir(path)\n",
    "    img_names = sorted(dir_list, key=lambda x: int(x.split(\".\")[0].split(\"_\")[1]))\n",
    "\n",
    "    batch_size = 15\n",
    "\n",
    "    number_of_batchs = len(img_names) // batch_size\n",
    "    output_path = path.split('\\\\')[0] + '\\depth'\n",
    "    \n",
    "    if not os.path.isdir(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    \n",
    "    for batch in range(number_of_batchs):\n",
    "        start = batch * batch_size\n",
    "        end = start + 15 if (start + 15) < len(img_names) else len(img_names)\n",
    "\n",
    "        images = [Image.open(path + \"\\\\\" + str(img_names[i])) for i in range(start, end)]\n",
    "\n",
    "        pixel_values = [feature_extractor(images= image , return_tensors=\"pt\").pixel_values for image in images]\n",
    "        pixel_values = torch.cat(pixel_values, dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(pixel_values)\n",
    "            predicted_depth = outputs.predicted_depth\n",
    "\n",
    "        # interpolate to original size\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "            predicted_depth.unsqueeze(1),\n",
    "            size=images[0].size[::-1],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        # visualize the prediction\n",
    "        predictions = torch.split(prediction, split_size_or_sections=1, dim=0)\n",
    "        for i, pred in enumerate(predictions):\n",
    "            output = pred.squeeze().cpu().numpy()\n",
    "            formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "            depth = Image.fromarray(formatted)\n",
    "\n",
    "            depth.save(output_path + '\\depth_'+ str(start + i) + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_to_video(path):\n",
    "        \n",
    "    # Dossier contenant les images numérotées\n",
    "    repo = path.split('\\\\')[0]\n",
    "    img_folder = repo + \"\\depth\\\\\"\n",
    "    \n",
    "    if not os.path.isdir(img_folder):\n",
    "        os.mkdir(img_folder)\n",
    "\n",
    "    # Obtenez la liste des noms de fichiers d'images dans le dossier\n",
    "    img_names = os.listdir(img_folder)\n",
    "\n",
    "    # Triez les noms de fichiers d'images par ordre numérique\n",
    "    img_names = sorted(img_names, key=lambda x: int(x.split(\".\")[0].split(\"_\")[1]))\n",
    "\n",
    "    # Obtenez la largeur et la hauteur de la première image\n",
    "    img_path = os.path.join(img_folder, img_names[0])\n",
    "    img = cv2.imread(img_path)\n",
    "    height, width, layers = img.shape\n",
    "\n",
    "    # Créez un objet VideoWriter pour enregistrer la vidéo\n",
    "    video_path = repo + \"\\depth_video.mp4\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(video_path, fourcc, 30, (width, height))\n",
    "\n",
    "    # Parcourez chaque image triée et ajoutez-la à la vidéo\n",
    "    for img_name in img_names:\n",
    "        img_path = os.path.join(img_folder, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        video.write(img)\n",
    "\n",
    "    # Fermez l'objet VideoWriter et affichez un message de confirmation\n",
    "    video.release()\n",
    "    print(\"La vidéo a été créée avec succès !\")\n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_tracking (original_video_path, depth_video_path) :\n",
    "        #finger sur une vidéo\n",
    "\n",
    "    # Set up video capture from default camera\n",
    "    cap = cv2.VideoCapture(original_video_path)\n",
    "    depth = cv2.VideoCapture(depth_video_path)\n",
    "\n",
    "    # Set up MediaPipe hand detection\n",
    "    mpHands = mp.solutions.hands\n",
    "    hands = mpHands.Hands()\n",
    "    mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "    # Initialize list of finger positions\n",
    "    finger_positions = []\n",
    "    images = []\n",
    "    # Main loop for video capture and hand detection\n",
    "    imageCompteur = 0\n",
    "    while cap.isOpened():\n",
    "        # Capture a frame from the camera\n",
    "        success, image = cap.read()\n",
    "\n",
    "        # Check if the frame was successfully read\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        if depth.isOpened():\n",
    "            # Capture a frame from the camera\n",
    "            success_depth, image_depth = depth.read()\n",
    "\n",
    "            # Check if the frame was successfully read\n",
    "            if not success_depth:\n",
    "                break\n",
    "\n",
    "            image = cv2.flip(image, 1)\n",
    "            image_depth = cv2.flip(image_depth, 1)\n",
    "\n",
    "            mask = image_depth > 220\n",
    "            sumMask = sum(sum(sum(mask))) \n",
    "\n",
    "            # Convert the color space of the image from BGR to RGB\n",
    "            imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Use MediaPipe to detect hand landmarks in the image\n",
    "            results = hands.process(imageRGB)\n",
    "\n",
    "            # Check if any hands were detected in the image\n",
    "            if results.multi_hand_landmarks:\n",
    "                # Loop through all detected hands\n",
    "                for handLms in results.multi_hand_landmarks:\n",
    "                    # Loop through all the landmarks of the current hand\n",
    "                    hasHeight = False\n",
    "                    for id, lm in enumerate(handLms.landmark):\n",
    "                        # Get the pixel coordinates of the landmark\n",
    "                        h, w, c = image.shape\n",
    "                        cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                        \n",
    "                        # If the current landmark is the tip of the index finger, add its position to the list\n",
    "                        if id == 8 :\n",
    "                            hasHeight = True\n",
    "                            if sumMask> 115000 : #image_depth[cy][cx][0] > 220 :\n",
    "                                finger_positions.append((cx, cy))\n",
    "                                cv2.circle(image, (cx, cy), 10, (255, 0, 255), cv2.FILLED)\n",
    "                                imageCompteur = 0 \n",
    "                            else : \n",
    "                                imageCompteur +=1\n",
    "\n",
    "                    if not hasHeight :\n",
    "                        imageCompteur +=1\n",
    "                    if imageCompteur >= 5 :\n",
    "                        finger_positions.append(\"stop\")\n",
    "                        imageCompteur = 0\n",
    "                        \n",
    "                    # Draw the landmarks and connections on the image using MediaPipe\n",
    "                    mpDraw.draw_landmarks(image, handLms, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "            # If there are any finger positions in the list, draw a curve passing through all of them\n",
    "            finger_positions.append(\"stop\")\n",
    "            if len(finger_positions) > 0:\n",
    "                #split array\n",
    "                curve = []\n",
    "                for i in finger_positions :\n",
    "                    if i != \"stop\" :\n",
    "                        curve.append(i)\n",
    "                    elif len(curve) > 0:\n",
    "                        curve = np.array(curve)\n",
    "                        cv2.polylines(image, [curve], False, (255, 0, 0), 3)\n",
    "                        curve = []\n",
    "            finger_positions.pop()\n",
    "            \n",
    "            # Display the image on the screen\n",
    "            cv2.imshow(\"Output\", image)\n",
    "            images.append(image)\n",
    "\n",
    "            # Check for the Esc key to stop the program\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    savePath = depth_video_path.split('\\\\')[0] + \"\\\\tracking.mp4\"\n",
    "    out = cv2.VideoWriter(savePath, fourcc, 30, (image.shape[1], image.shape[0]))\n",
    "    print(\"video save at: \" + savePath)\n",
    "\n",
    "    # Main loop for writing video\n",
    "    for img in images:\n",
    "        out.write(img)\n",
    "\n",
    "    out.release()\n",
    "    # Release the video capture object and close all windows\n",
    "    cap.release()\n",
    "    depth.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video\\init-opencv\n",
      "La vidéo a été créée avec succès !\n",
      "video save at: video\\tracking.mp4\n"
     ]
    }
   ],
   "source": [
    "#main \n",
    "output = 'video\\init.mp4'\n",
    "recordVideo(output)\n",
    "frame_dir = video_to_frames(output)\n",
    "\n",
    "#frame_dir = \"video\\WIN_20230309_14_27_22_Pro-opencv\"\n",
    "#frame_dir = \"video\\source-opencv\"\n",
    "frame_to_depth_frame(frame_dir)\n",
    "video_path = frames_to_video(output)\n",
    "video_tracking(output, video_path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
